{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFMqjt4DwHUR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME']=\"simransohal\"\n",
        "os.environ['KAGGLE_KEY']=\"3aa7697cd9a4ba2a3715c4a29c0f0cb6\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d grassknoted/asl-alphabet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLOO6Btew4JP",
        "outputId": "58abd2d5-236c-4ece-d68a-457f89655f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asl-alphabet.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcuu6XXWw-S1",
        "outputId": "f9a23415-e1c5-4d8f-81f7-bda54b617729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34masl_alphabet_test\u001b[0m/  \u001b[01;34masl_alphabet_train\u001b[0m/  asl-alphabet.zip  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('asl-alphabet.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "id": "Rarc6yEjyJRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XodsxbryQTe",
        "outputId": "a6caa076-4609-45d0-b7b7-85f1599e8d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34masl_alphabet_test\u001b[0m/  \u001b[01;34masl_alphabet_train\u001b[0m/  asl-alphabet.zip  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "gysxU2qNyZbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ba71D5Kx1jjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=models.resnet18(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax7Mujtp12li",
        "outputId": "88d27d1c-f0d4-4abf-ce82-14da6fd26cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8WHAYP12-Q9",
        "outputId": "5bf735a4-9ef3-428f-dbed-faeaf7642608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34masl_alphabet_test\u001b[0m/  \u001b[01;34masl_alphabet_train\u001b[0m/  asl-alphabet.zip  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the number of input features in fully connected layer\n",
        "#Model is to be classified in just three layers\n",
        "#Created a new layer and replaced the fully connected layer with it\n",
        "num_classes=3\n",
        "features=model.fc.in_features\n",
        "model.fc=nn.Linear(features,num_classes)"
      ],
      "metadata": {
        "id": "eWXRoNNCTFvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=model.to(d)"
      ],
      "metadata": {
        "id": "MTZeAEtDWUlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformations=transforms.Compose([transforms.Resize(256),transforms.CenterCrop(224),transforms.ToTensor(),transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])"
      ],
      "metadata": {
        "id": "S59KGVITWghg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=datasets.ImageFolder(r\"asl_alphabet_train\",transform=transformations)"
      ],
      "metadata": {
        "id": "WWNV0DMKgqx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=datasets.ImageFolder(r\"asl_alphabet_test\",transform=transformations)"
      ],
      "metadata": {
        "id": "Qvjisi2Iq2y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=Adam(model.parameters(),lr=0.001)\n",
        "criterion=nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "oIa9KJiIrbHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "loader_train=DataLoader(data,batch_size=128,shuffle=True )\n",
        "loader_test=DataLoader(data,batch_size=128,shuffle=True)"
      ],
      "metadata": {
        "id": "aDSQqxGqsCAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=5\n",
        "for epoch in range(n):\n",
        "  #Going through the batches of train images\n",
        "  for images, labels in loader_train:\n",
        "    #transfering labels and images to gpu\n",
        "    images,labels=images.to(d),labels.to(d)\n",
        "    #resets gradient of all batches, because by default the gradients are accumulated \n",
        "    optimizer.zero_grad()\n",
        "    #passes images through model to obtain predicted output\n",
        "    result=model(images)\n",
        "    #predicting loss\n",
        "    loss=criterion(result,labels)\n",
        "    #computes gradient of loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{n}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDoo8KzntUy3",
        "outputId": "f7e73f3e-43d5-4921-c872-bca196c0de73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 1.1570\n",
            "Epoch [1/5], Loss: 0.7272\n",
            "Epoch [1/5], Loss: 0.5263\n",
            "Epoch [1/5], Loss: 0.2458\n",
            "Epoch [1/5], Loss: 0.1857\n",
            "Epoch [1/5], Loss: 0.0303\n",
            "Epoch [1/5], Loss: 0.0136\n",
            "Epoch [1/5], Loss: 0.0061\n",
            "Epoch [1/5], Loss: 0.0039\n",
            "Epoch [1/5], Loss: 0.0027\n",
            "Epoch [1/5], Loss: 0.0020\n",
            "Epoch [1/5], Loss: 0.0023\n",
            "Epoch [1/5], Loss: 0.0015\n",
            "Epoch [1/5], Loss: 0.0012\n",
            "Epoch [1/5], Loss: 0.0015\n",
            "Epoch [1/5], Loss: 0.0010\n",
            "Epoch [1/5], Loss: 0.0008\n",
            "Epoch [1/5], Loss: 0.0004\n",
            "Epoch [1/5], Loss: 0.0004\n",
            "Epoch [1/5], Loss: 0.0003\n",
            "Epoch [1/5], Loss: 0.0003\n",
            "Epoch [1/5], Loss: 0.0003\n",
            "Epoch [1/5], Loss: 0.0002\n",
            "Epoch [1/5], Loss: 0.0002\n",
            "Epoch [1/5], Loss: 0.0002\n",
            "Epoch [1/5], Loss: 0.0002\n",
            "Epoch [1/5], Loss: 0.0002\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0001\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n",
            "Epoch [1/5], Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total=0\n",
        "  for image,label in loader_test:\n",
        "    images,label=images.to(d),labels.to(d)\n",
        "    result=model(images)\n",
        "    _,predicted=torch.max(result.data,1)\n",
        "    total=total+labels.size(0)\n",
        "    correct=correct+(predicted==labels).sum().items()\n",
        "    print(f'Accuracy {100 * correct / total:.2f}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5nCWbn99pqFN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}